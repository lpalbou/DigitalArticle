#!/bin/bash
set -e

echo "ü§ñ Ollama Container - Entrypoint"
echo "=================================="

# Start Ollama service in background
echo "üöÄ Starting Ollama service..."
/bin/ollama serve &
OLLAMA_PID=$!

# Wait for Ollama to be ready (using ollama cli instead of curl)
echo "‚è≥ Waiting for Ollama to be ready..."
for i in {1..30}; do
    if ollama list > /dev/null 2>&1; then
        echo "‚úÖ Ollama service is ready!"
        break
    fi
    if [ $i -eq 30 ]; then
        echo "‚ùå Ollama failed to start after 60 seconds"
        exit 1
    fi
    sleep 2
done

# Read model from config.json (mounted from host)
if [ -f /config.json ]; then
    echo ""
    echo "üìù Reading configuration from /config.json..."

    MODEL=$(python3 -c "
import json
try:
    with open('/config.json') as f:
        config = json.load(f)
        model = config.get('llm', {}).get('model', 'unknown')
        print(model)
except Exception as e:
    print('unknown')
    import sys
    print(f'Error reading config: {e}', file=sys.stderr)
" 2>&1)

    echo "  Model: $MODEL"
    echo ""

    if [ "$MODEL" != "unknown" ]; then
        # Check if model already exists using ollama list
        echo "üîç Checking if model '$MODEL' is available..."

        MODEL_EXISTS=$(ollama list 2>/dev/null | grep -q "^${MODEL}" && echo "yes" || echo "no")

        if [ "$MODEL_EXISTS" = "yes" ]; then
            echo "‚úÖ Model '$MODEL' is already available!"
        else
            echo "üì• Model '$MODEL' not found - downloading now..."
            echo ""
            echo "‚ÑπÔ∏è  Model sizes (approximate download times):"
            echo "   - qwen3-coder:4b   ‚Üí 2.6GB   (~2-5 min)"
            echo "   - qwen3-coder:8b   ‚Üí 5GB     (~5-10 min)"
            echo "   - qwen3-coder:14b  ‚Üí 8.5GB   (~10-15 min)"
            echo "   - qwen3-coder:30b  ‚Üí 17GB    (~15-30 min)"
            echo ""
            echo "‚è≥ Downloading model (this may take a while)..."
            echo ""

            # Pull model with progress
            ollama pull "$MODEL"

            echo ""
            echo "‚úÖ Model '$MODEL' downloaded successfully!"
        fi
    fi
else
    echo "‚ö†Ô∏è  /config.json not found - skipping model download"
    echo "   Models can be pulled manually: ollama pull <model>"
fi

echo ""
echo "=================================================="
echo "Ollama Ready"
echo "=================================================="
echo "  Available at: http://localhost:11434"
echo "  Models loaded: $(ollama list | wc -l) model(s)"
echo "=================================================="
echo ""

# Keep Ollama service running in foreground
wait $OLLAMA_PID
