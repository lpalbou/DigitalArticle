# Supervisord configuration for Digital Article unified container
# This configuration runs Ollama, Backend, and Nginx in a single container

[supervisord]
nodaemon=true
logfile=/var/log/supervisor/supervisord.log
pidfile=/var/run/supervisord.pid
childlogdir=/var/log/supervisor
loglevel=info
user=root

[program:ollama]
command=/usr/local/bin/ollama serve
autostart=true
autorestart=true
priority=10
stdout_logfile=/var/log/supervisor/ollama.log
stderr_logfile=/var/log/supervisor/ollama_err.log
environment=OLLAMA_MODELS="%(ENV_OLLAMA_MODELS)s",OLLAMA_HOST="0.0.0.0:11434"
user=root
startsecs=5
stopwaitsecs=30

[program:backend]
command=/opt/venv/bin/python -m uvicorn backend.app.main:app --host 0.0.0.0 --port 8000
directory=/app
autostart=false  ; Started by entrypoint after Ollama ready (or immediately for external providers)
autorestart=true
priority=20
stdout_logfile=/var/log/supervisor/backend.log
stderr_logfile=/var/log/supervisor/backend_err.log
environment=PATH="/opt/venv/bin:/usr/local/bin:/usr/bin:/bin",PYTHONPATH="/app",LLM_PROVIDER="%(ENV_LLM_PROVIDER)s",LLM_MODEL="%(ENV_LLM_MODEL)s",OLLAMA_BASE_URL="%(ENV_OLLAMA_BASE_URL)s",LMSTUDIO_BASE_URL="%(ENV_LMSTUDIO_BASE_URL)s",OPENAI_API_KEY="%(ENV_OPENAI_API_KEY)s",ANTHROPIC_API_KEY="%(ENV_ANTHROPIC_API_KEY)s",HUGGINGFACE_TOKEN="%(ENV_HUGGINGFACE_TOKEN)s",PYTHONUNBUFFERED="1",NOTEBOOKS_DIR="%(ENV_NOTEBOOKS_DIR)s",WORKSPACE_DIR="%(ENV_WORKSPACE_DIR)s"
user=root
startsecs=10
stopwaitsecs=30

[program:nginx]
command=/usr/sbin/nginx -g "daemon off;"
autostart=false  ; Started by entrypoint after backend ready
autorestart=true
priority=30
stdout_logfile=/var/log/supervisor/nginx.log
stderr_logfile=/var/log/supervisor/nginx_err.log
user=root
startsecs=3
stopwaitsecs=10
