# ============================================
# Dockerfile for NVIDIA GPU (CUDA)
# ============================================
# Optimized for: Systems with NVIDIA GPUs
# Base: nvidia/cuda:12.2.2-runtime-ubuntu22.04
# Requirements: Host must have NVIDIA Drivers
# and NVIDIA Container Toolkit installed.
# Run with: --gpus all
# ============================================

# ============================================
# Stage 1: Frontend Build
# ============================================
FROM node:20-alpine AS frontend-builder

WORKDIR /build
COPY frontend/package*.json ./
RUN npm ci
COPY frontend/ .
RUN npm run build

# ============================================
# Stage 2: Backend Build (Ubuntu-based)
# ============================================
# We use the same base OS (Ubuntu 22.04) for building to ensure
# C-extensions (like pandas/numpy) are compatible with runtime.
FROM ubuntu:22.04 AS backend-builder

ENV DEBIAN_FRONTEND=noninteractive

# Install build dependencies and Python 3.12 (cmake/swig/zlib for tellurium/SBML)
RUN apt-get update && apt-get install -y --no-install-recommends \
    software-properties-common gpg-agent \
    gcc g++ libpq-dev git curl cmake make swig zlib1g-dev \
    && add-apt-repository ppa:deadsnakes/ppa \
    && apt-get update \
    && apt-get install -y python3.12 python3.12-venv python3.12-dev \
    && rm -rf /var/lib/apt/lists/*

# Create virtual environment
RUN python3.12 -m venv /opt/venv
ENV PATH="/opt/venv/bin:$PATH"

# Install Python dependencies
WORKDIR /build
COPY backend/pyproject.toml ./
COPY backend/app ./app
# Needed for backend dynamic version (single source of truth: `digitalarticle/_version.py`)
COPY digitalarticle/_version.py ./digitalarticle/_version.py
# CFLAGS to allow building older C extensions with newer GCC
RUN pip install --no-cache-dir --upgrade pip && \
    CFLAGS="-Wno-error=incompatible-pointer-types" pip install --no-cache-dir .

# ============================================
# Stage 3: Runtime Image (NVIDIA CUDA)
# ============================================
FROM nvidia/cuda:12.2.2-runtime-ubuntu22.04

LABEL org.opencontainers.image.title="Digital Article - NVIDIA CUDA"
LABEL org.opencontainers.image.description="GPU-accelerated execution with NVIDIA CUDA"

ENV DEBIAN_FRONTEND=noninteractive

# Install runtime dependencies & Python 3.12
RUN apt-get update && apt-get install -y --no-install-recommends \
    software-properties-common \
    libpq-dev \
    libgomp1 \
    nginx \
    supervisor \
    curl ca-certificates \
    # Install Python 3.12
    && add-apt-repository ppa:deadsnakes/ppa \
    && apt-get update \
    && apt-get install -y python3.12 python3.12-venv \
    && rm -rf /var/lib/apt/lists/*

# Set python (not python3) to point to python3.12 to avoid breaking system tools
RUN update-alternatives --install /usr/bin/python python /usr/bin/python3.12 1

# Copy Ollama binary
COPY --from=ollama/ollama:latest /bin/ollama /usr/local/bin/ollama

# Copy Python virtual environment from backend-builder
COPY --from=backend-builder /opt/venv /opt/venv
ENV PATH="/opt/venv/bin:$PATH"

# Copy frontend build
COPY --from=frontend-builder /build/dist /usr/share/nginx/html

# Application Setup
WORKDIR /app
COPY backend/ ./backend/
COPY config.json ./config.json

# Copy default system personas
COPY data/personas/system/ ./data/personas/system/

COPY docker/monolithic/nginx.conf /etc/nginx/conf.d/default.conf
RUN rm -f /etc/nginx/sites-enabled/default /etc/nginx/sites-available/default 2>/dev/null || true
COPY docker/monolithic/supervisord.conf /etc/supervisor/conf.d/digitalarticle.conf
COPY docker/monolithic/entrypoint.sh /entrypoint.sh
RUN chmod +x /entrypoint.sh

# Create directories
RUN mkdir -p /app/data/notebooks /app/data/workspace /app/data/personas/custom /app/logs /models/ollama /models/huggingface /var/log/supervisor

# ============================================
# BUILD ARGUMENTS
# Customize at build time with --build-arg
# Example: docker build --build-arg OPENAI_COMPATIBLE_BASE_URL=http://myserver:8080/v1 ...
# ============================================
ARG LLM_PROVIDER=openai-compatible
ARG LLM_MODEL=

# Default to host.docker.internal (access host machine from container)
# For Linux without Docker Desktop: use --add-host=host.docker.internal:host-gateway
ARG OLLAMA_BASE_URL=http://host.docker.internal:11434
ARG LMSTUDIO_BASE_URL=http://host.docker.internal:1234/v1
ARG VLLM_BASE_URL=http://host.docker.internal:8000/v1
ARG OPENAI_COMPATIBLE_BASE_URL=http://host.docker.internal:1234/v1

# Environment
# LLM Configuration (can be overridden at runtime with -e)
# Note: API keys (OPENAI_API_KEY, ANTHROPIC_API_KEY, HUGGINGFACE_TOKEN, VLLM_API_KEY, OPENAI_COMPATIBLE_API_KEY)
# should be passed at runtime via -e, not baked into image
ENV LLM_PROVIDER=${LLM_PROVIDER} \
    LLM_MODEL=${LLM_MODEL} \
    NOTEBOOKS_DIR=/app/data/notebooks \
    WORKSPACE_DIR=/app/data/workspace \
    OLLAMA_MODELS=/models/ollama \
    HF_HOME=/models/huggingface \
    OLLAMA_BASE_URL=${OLLAMA_BASE_URL} \
    LMSTUDIO_BASE_URL=${LMSTUDIO_BASE_URL} \
    VLLM_BASE_URL=${VLLM_BASE_URL} \
    OPENAI_COMPATIBLE_BASE_URL=${OPENAI_COMPATIBLE_BASE_URL} \
    PYTHONUNBUFFERED=1 \
    LOG_LEVEL=INFO \
    DIGITAL_ARTICLE_VARIANT="NVIDIA CUDA (GPU)" \
    NVIDIA_VISIBLE_DEVICES=all \
    NVIDIA_DRIVER_CAPABILITIES=compute,utility

EXPOSE 80

HEALTHCHECK --interval=30s --timeout=10s --start-period=120s --retries=3 \
    CMD curl -f http://localhost:80/health || exit 1

ENTRYPOINT ["/entrypoint.sh"]

